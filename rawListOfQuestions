1.	INTERVIEW QUESTIONS WITH INTERVIEW ANSWERS
2.	(500+ Question and Answers)
3.	Topic: Data Science, Machine Learning and Deep Learning
4.	By Karthik Kumar Billa, https://www.linkedin.com/in/karthik-kumar-billa/



5.	 
6.	Perfect Phrases for Perfect Interviews
7.	“Tell me about yourself.”

8.	Formula: “Tell me briefly about your professional experience and the relevant qualities that make you a strong candidate for this job.” Personal qualities + Skills + accomplishments;
9.	What are the words that describe you?
10.	Reliable: Never miss deadlines
11.	Communicator: Explain complex things in simple terms to all the people involved in a project
12.	Organized: Take notes,  
13.	Team player: 
14.	Efficient: works well and quickly and is good at organizing work in the way the gets best results
15.	Hard at work: working with a lot of effort
16.	Productive and Committed to work place
17.	Clever: good at learning things
18.	Results oriented: I am results-oriented, constantly checking in with the goal to determine how close or how far away we are and what it will take to make it happen. 
19.	Weaknesses:
20.	Focus too much on details: My greatest weakness is that I sometimes focus too much on the details of a project and spend too much time analyzing the finer points. I’ve been striving to improve in this area by checking in with myself at regular intervals and giving myself a chance to re-focusing on the bigger picture. That way I can still ensure quality without getting so caught up in the details that it affects my productivity or the team’s ability to meet the deadline.

21.	Trouble saying NO: I end up taking on more than I can handle. This makes me burn out at times. I am planning to overcome this by using my Project Management skills to visualize how much work I can handle.	


22.	MATHEMATICS PRIMER
23.	Linear Algebra
24.	Define Point/Vector (2-D, 3-D, and n-D)?
25.	How to calculate Dot product and angle between 2 vectors?
26.	Define Projection, unit vector?
27.	Equation of a line (2-D), plane (3-D) and Hyperplane (n-D)?
28.	Distance of a point from a plane/Hyperplane, half-spaces?
29.	Equation of a circle (2-D), sphere (3-D) and hyper sphere (n-D)?
30.	Equation of an ellipse (2-D), ellipsoid (3-D) and hyper ellipsoid (n-D)?
31.	Square, Rectangle, Hyper-cube and Hyper-cuboid?

32.	Probability and Statistics
33.	What are Random variables: discrete and continuous?
34.	A random variable is a variable whose value is unknown.
35.	Define Outliers (or) extreme points?
36.	What is PDF?
37.	What is CDF?
38.	Explain about 1-std-dev, 2-std-dev, 3-std-dev range?
39.	What is Symmetric distribution, Skewness and Kurtosis?
40.	How to do Standard normal variate (z) and standardization?
41.	What is Kernel density estimation?
42.	Importance of Sampling distribution & Central Limit theorem.
43.	Importance of Q-Q Plot: Is a given random variable Gaussian distributed?
44.	What is Uniform Distribution and random number generators
45.	What Discrete and Continuous Uniform distributions?
46.	How to randomly sample data points?
47.	Explain about Bernoulli and Binomial distribution?
48.	What is Log-normal and power law distribution?
49.	What is Power-law & Pareto distributions: PDF, examples
50.	Explain about Box-Cox/Power transform?
51.	What is Co-variance?
52.	Importance of Pearson Correlation Coefficient?
53.	Importance Spearman Rank Correlation Coefficient?
54.	Correlation vs Causation?
55.	What is Confidence Interval?
56.	Confidence Interval vs Point estimate?
57.	Explain about Hypothesis testing?
58.	Define Hypothesis Testing methodology, Null-hypothesis, test-statistic, p-value?
59.	How to do K-S Test for similarity of two distributions?

60.	Interview Questions on Probability and statistics
61.	What is a random variable?
62.	What are the conditions for a function to be a probability mass function?(http://www.statisticshowto.com/probability-mass-function-pmf/)
63.	What are the conditions for a function to be a probability density function ?(Covered in our videos)
64.	What is conditional probability? 
65.	State the Chain rule of conditional probabilities?(https://en.wikipedia.org/wiki/Chain_rule_(probability))
66.	What are the conditions for independence and conditional independence of two random variables?(https://math.stackexchange.com/questions/22407/independence-and-conditional-independence-between-random-variables)
67.	What are expectation, variance and covariance?(Covered in our videos)
68.	Compare covariance and independence?(https://stats.stackexchange.com/questions/12842/covariance-and-independence)
69.	What is the covariance for a vector of random variables?(https://math.stackexchange.com/questions/2697376/find-the-covariance-matrix-of-a-vector-of-random-variables)
70.	What is a Bernoulli distribution? 
71.	What is a normal distribution?
72.	What is the central limit theorem?
73.	Write the formula for Bayes rule?
74.	If two random variables are related in a deterministic way, how are the PDFs related?
75.	What is Kullback-Leibler (KL) divergence?
76.	Can KL divergence be used as a distance measure?
77.	What is Bayes’ Theorem? How is it useful in a machine learning context?
78.	Why is “Naive” Bayes naive?
79.	What’s a Fourier transform?
80.	What is the difference between covariance and correlation?
81.	Is it possible capture the correlation between continuous and categorical variable? If yes, how?
82.	What is the Box-Cox transformation used for?
83.	What does P-value signify about the statistical data?
84.	A test has a true positive rate of 100% and false positive rate of 5%. There is a population with a 1/1000 rate of having the condition the test identifies. Considering a positive test, what is the probability of having that condition?
85.	How you can make data normal using Box-Cox transformation?
86.	Explain about the box cox transformation in regression models.
87.	What is the difference between skewed and uniform distribution?
88.	What do you understand by Hypothesis in the content of Machine Learning?
89.	How will you find the correlation between a categorical variable and a continuous variable?
90.	How to sample from a Normal Distribution with known mean and variance?

91.	Dimensionality Reduction
92.	What is dimensionality reduction? 
93.	The process of reducing dimensionality from high to low ensuring least loss of information. The dimensionality of a data set is the number of random variables that define each data point. Through dimensionality reduction we are obtaining a smaller set of random variables which can equally represent the data set.  We generally reach the smallest number of random variables that  preserve the largest variance
94.	Explain Principal Component Analysis?
95.	Importance of PCA?
96.	Limitations of PCA?
97.	What is t-SNE?
98.	What is Crowding problem?
99.	How to apply t-SNE and interpret its output?

100.	Interview Questions on Dimensionality Reduction
101.	You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.)(https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
102.	Is rotation necessary in PCA? If yes, Why? https://google-interview-hacks.blogspot.com/2017/04/is-rotation-necessary-in-pca-if-yes-why.html
103.	You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?(https://www.linkedin.com/pulse/questions-machine-learning-statistics-can-you-answer-saraswat/)


104.	MACHINE LEARNING
105.	Classification and Regression Models: kNN Revision Questions
106.	Explain about K-Nearest Neighbors?
107.	KNN is a non-parametric, lazy algorithm. It stores all available cases and classifies new cases based on a similarity measure. 
108.	Failure cases of KNN?
109.	Outliers, Randomized data 
110.	Define Distance measures: Euclidean(L2) , Manhattan(L1), Minkowski,  Hamming
111.	Euclidean distance: straight-line distance between two points; 

112.	What is Cosine Distance & Cosine Similarity?
113.	How to measure the effectiveness of k-NN?
114.	Limitations of KNN?
115.	How to handle Overfitting and Underfitting in KNN?
116.	Need for Cross validation?
117.	What is K-fold cross validation?
118.	What is Time based splitting?
119.	Explain k-NN for regression?
120.	Weighted k-NN?
121.	How to build a kd-tree?
122.	Find nearest neighbors using kd-tree
123.	What is Locality sensitive Hashing (LSH)?(Hashing vs LSH?
124.	LSH for cosine similarity?
125.	LSH for Euclidean distance?
126.	Interview Questions on kNN
127.	In k-means or kNN, we use Euclidean distance to calculate the distance between nearest neighbors. Why not Manhattan distance ?(https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/)
128.	How to test and know whether or not we have overfitting problem?(https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/how-to-determine-overfitting-and-underfitting/)
129.	How is kNN different from k-means clustering?(https://stats.stackexchange.com/questions/56500/what-are-the-main-differences-between-k-means-and-k-nearest-neighbours)
130.	Can you explain the difference between a Test Set and a Validation Set?(https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo)
131.	How can you avoid overfitting in KNN?(https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/how-to-determine-overfitting-and-underfitting/)
132.	External Resources: 1.https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/

133.	Classification algorithms in various situations
134.	What is Imbalanced and balanced dataset?	
135.	Define Multi-class classification?
136.	Explain Impact of Outliers?
137.	What is Local Outlier Factor?
138.	What is k-distance (A), N(A)
139.	Define reachability-distance(A, B)?
140.	What is Local-reachability-density(A)?
141.	Define LOF(A)?
142.	Impact of Scale & Column standardization?
143.	What is Interpretability?
144.	Handling categorical and numerical features?
145.	Handling missing values by imputation?
146.	Bias-Variance tradeoff?

147.	Performance Measurement of Models:
148.	What is Accuracy?
149.	Explain about Confusion matrix, TPR, FPR, FNR, TNR?
150.	What do you understand about Precision & recall, F1-score? How would you use it?
151.	What is the ROC Curve and what is AUC (a.k.a. AUROC)?
152.	What is Log-loss and how it helps to improve performance?
153.	Explain about R-Squared/ Coefficient of determination.
154.	Explain about Median absolute deviation (MAD)? Importance of MAD?
155.	Define Distribution of errors?
156.	Interview Questions on Performance Measurement of Models:
157.	Which is more important to you– model accuracy, or model performance?
158.	Can you cite some examples where a false positive is important than a false negative?
159.	Can you cite some examples where a false negative important than a false positive?
160.	Can you cite some examples where both false positive and false negatives are equally important?
161.	What is the most frequent metric to assess model accuracy for classification problems?
162.	Why is Area Under ROC Curve (AUROC) better than raw accuracy as an out-of- sample evaluation metric?

163.	Naive Bayes
164.	What is Conditional probability?
165.	Define Independent vs Mutually exclusive events?
166.	Explain Bayes Theorem with example?
167.	How to apply Naive Bayes on Text data?
168.	What is Laplace/Additive Smoothing?
169.	Explain Log-probabilities for numerical stability?
170.	In Naive Bayes how to handle Bias and Variance tradeoff?
171.	What Imbalanced data?
172.	What is Outliers and how to handle outliers?
173.	How to handle Missing values?
174.	How to Handling Numerical features (Gaussian NB) 
175.	Define Multiclass classification?
176.	Logistic Regression and Linear Regression
177.	Explain about Logistic regression?
178.	What is sigmoid function & Squashing?
179.	Explain about Optimization problem in logistic regression.
180.	Importance of Weight vector in logistic regression.
181.	L2 Regularization: Overfitting and Underfitting.
182.	L1 regularization and sparsity. 
183.	What is Probabilistic Interpretation: Gaussian Naive Bayes ?
184.	Explain about Hyperparameter search: Grid Search and Random Search ?
185.	What is Column Standardization.?
186.	Explain about Collinearity of features?
187.	Find Train & Run time space and time complexity of Logistic regression?
188.	Interview Questions on Logistic Regression and Linear Regression
189.	After analysing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s true? Without losing any information, can you still build a better model?(https://google-interview-hacks.blogspot.in/2017/04/after-analyzing-model-your-manager-has.html)
190.	What are the basic assumptions to be made for linear regression?(https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1-2-copy-8/)
191.	What is the difference between stochastic gradient descent (SGD) and gradient descent (GD)?(https://stats.stackexchange.com/questions/317675/gradient-descent-gd-vs-stochastic-gradient-descent-sgd)
192.	When would you use GD over SDG, and vice-versa?(https://elitedatascience.com/machine-learning-interview-questions-answers)
193.	How do you decide whether your linear regression model fits the data?(https://www.researchgate.net/post/What_statistical_test_is_required_to_assess_goodness_of_fit_of_a_linear_or_nonlinear_regression_equation)
194.	Is it possible to perform logistic regression with Microsoft Excel?(https://www.youtube.com/watch?v=EKRjDurXau0)
195.	When will you use classification over regression?(https://www.quora.com/When-will-you-use-classification-over-regression)
196.	Why isn't Logistic Regression called Logistic Classification?(Refer :https://stats.stackexchange.com/questions/127042/why-isnt-logistic-regression-called-logistic-classification/127044)

197.	External Resources: 
198.	https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/ 
199.	https://www.listendata.com/2017/03/predictive-modeling-interview-questions.html 
200.	https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/ 
201.	https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/ 
202.	https://www.listendata.com/2018/03/regression-analysis.html
203.	Support Vector Machine
204.	Explain About SVM? 
205.	What is Hinge Loss?
206.	Dual form of SVM formulation?
207.	What is Kernel trick?
208.	What is Polynomial kernel?
209.	What is RBF-Kernel?
210.	Explain about Domain specific Kernels.
211.	Find Train and run time complexities for SVM?
212.	Explain about SVM Regression?
213.	When to use Logistic Regression over SVM? https://vitalflux.com/machine-learning-use-logistic-regression-vs-svm/
214.	What are the advantage and disadvantage of Logistic Regression and SVM?
215.	Interview Questions on SVM
216.	Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa.(https://datascience.stackexchange.com/questions/6838/when-to-use-random-forest-over-svm-and-vice-versa)
217.	What is convex hull ?(https://en.wikipedia.org/wiki/Convex_hull)
218.	What is a large margin classifier?
219.	Why SVM is an example of a large margin classifier?
220.	SVM being a large margin classifier, is it influenced by outliers? (Yes, if C is large, otherwise not)
221.	What is the role of C in SVM?
222.	In SVM, what is the angle between the decision boundary and theta?
223.	What is the mathematical intuition of a large margin classifier?
224.	What is a kernel in SVM? Why do we use kernels in SVM?
225.	What is a similarity function in SVM? Why it is named so?
226.	How are the landmarks initially chosen in an SVM? How many and where?
227.	Can we apply the kernel trick to logistic regression? Why is it not used in practice then?
228.	What is the difference between logistic regression and SVM without a kernel? (Only in implementation – one is much more efficient and has good optimization packages)
229.	How does the SVM parameter C affect the bias/variance trade off? (Remember C = 1/lambda; lambda increases means variance decreases)
230.	How does the SVM kernel parameter sigma^2 affect the bias/variance trade off?
231.	Can any similarity function be used for SVM? (No, have to satisfy Mercer’s theorem)
232.	Logistic regression vs. SVMs: When to use which one? ( Let's say n and m are the number of features and training samples respectively. If n is large relative to m use log. Reg. or SVM with linear kernel, If n is small and m is intermediate, SVM with Gaussian kernel, If n is small and m is massive, Create or add more features then use log. Reg. or SVM without a kernel)
233.	What is the difference between supervised and unsupervised machine learning?
234.	External Resources: 1. https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/

235.	Decision Trees
236.	How to Building a decision Tree?
237.	What is Entropy? 
238.	What is information Gain?
239.	What is Gini Impurity?
240.	How to Constructing a DT?
241.	Importance of Splitting numerical features?
242.	How to handle Overfitting and Underfitting in DT?
243.	What are Train and Run time complexity for DT?
244.	How to implement Regression using Decision Trees?
245.	Interview Questions on Decision Trees
246.	You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?(Refer :https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
247.	Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?(Refer:https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
248.	External Resources: 1. https://vitalflux.com/decision-tree-algorithm-concepts-interview-question
249.	Ensemble Models:
250.	What are ensembles?
251.	What is Bootstrapped Aggregation (Bagging)?
252.	Explain about Random Forest and their construction?
253.	Explain about Boosting?
254.	What are Residuals, Loss functions and gradients ?
255.	Explain about Gradient Boosting?
256.	What is Regularization by Shrinkage?
257.	Explain about XGBoost?
258.	Explain about AdaBoost?
259.	How do you implement Stacking models?
260.	Explain about cascading classifiers.

261.	Clustering:
262.	What is K-means? How can you select K for K-means?
263.	How is KNN different from k-means clustering?
264.	Explain about Hierarchical clustering?
265.	Limitations of Hierarchical clustering?
266.	Time complexity of Hierarchical clustering?
267.	Explain about DBSCAN?
268.	Advantages and Limitations of DBSCAN?
269.	Recommender Systems and Matrix Factorization:
270.	Explain about Content based and Collaborative Filtering?
271.	What is PCA, SVD? 
272.	What is NMF?
273.	How to do MF for Collaborative filtering?
274.	How to do MF for feature engineering?
275.	Explain relation between Clustering and MF?
276.	What is Hyperparameter tuning?
277.	Explain about Cold Start problem?
278.	How to solve Word Vectors using MF?
279.	Explain about Eigen faces?
280.	Interview Questions on Recommender Systems and Matrix Factorization:
281.	How would you implement a recommendation system for our company’s users?(https://www.infoworld.com/article/3241852/machine-learning/how-to-implement-a-recommender-system.html)
282.	How would you approach the “Netflix Prize” competition?(Refer http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/)
283.	‘People who bought this, also bought…’ recommendations seen on Amazon is a result of which algorithm?(Please refer Apparel recommendation system case study, Refer: https://measuringu.com/affinity-analysis/)
284.	Basics of Natural Language Processing (NLP):
285.	Explain about Bag of Words?
286.	Explain about Text Preprocessing: Stemming, Stop-word removal, Tokenization, Lemmatization.
287.	Explain about uni-gram, bi-gram, n-grams?
288.	What is tf-idf (term frequency- inverse document frequency)?
289.	Why use log in IDF?
290.	Explain about Word2Vec?
291.	Explain about Avg-Word2Vec, tf-idf weighted Word2Vec?
292.	Explain about Multi-Layered Perceptron (MLP)?
293.	How to train a single-neuron model?
294.	How to Train an MLP using Chain rule?
295.	How to Train an MLP using Memoization?
296.	Explain about Backpropagation algorithm?
297.	Describe about Vanishing and Exploding Gradient problem?
298.	Explain about Bias-Variance tradeoff in neural Networks?
299.	Deep Learning:
300.	What is sampled softmax?
301.	Why is it difficult to train a RNN with SGD?
302.	How do you tackle the problem of exploding gradients? (By gradient clipping)
303.	What is the problem of vanishing gradients? (RNN doesn't tend to remember much things from the past)
304.	How do you tackle the problem of vanishing gradients? (By using LSTM)
305.	Explain the memory cell of a LSTM. (LSTM allows forgetting of data and using long memory when appropriate.)
306.	What type of regularization do one use in LSTM?
307.	What is the problem with sigmoid during backpropagation? (Very small, between 0.25 and zero.)
308.	What is transfer learning?
309.	What is Backpropagation through time? (BPTT)
310.	What is the difference between LSTM and GRU?
311.	Explain Gradient Clipping.
312.	Adam and RMSProp adjust the size of gradients based on previously seen gradients. Do they inherently perform gradient clipping? If no, why?
313.	External sources https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/
314.	 
315.	Questions:
316.	You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.)[Reference: https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/]
317.	Is rotation necessary in PCA? If yes, Why? What will happen if you don’t rotate the components?[Reference :https://www.quora.com/Is-rotation-necessary-in-PCA-If-yes-why-What-will-happen-if-you-don%E2%80%99t-rotate-the-components]
318.	You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?[Reference :https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/]
319.	You are given a data set on cancer detection. You’ve build a classification model and achieved an accuracy of 96%. Why shouldn’t you be happy with your model performance? What can you do about it?[Reference:https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/]
320.	You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?[https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/]
321.	You are assigned a new project which involves helping a food delivery company save more money. The problem is, company’s delivery team aren’t able to deliver food on time. As a result, their customers get unhappy. And, to keep them happy, they end up delivering food for free. Which machine learning algorithm can save them?
322.	You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?
323.	You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?
324.	After spending several hours, you are now anxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could perform better than benchmark score. Finally, you decided to combine those models. Though, ensembled .
325.	Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?
326.	You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven’t you trained your model perfectly?
327.	You’ve got a data set to work having p (no. of variable) > n (no. of observation). Why is OLS as bad option to work with? Which techniques would be best to use? Why?
328.	You have built a multiple regression model. Your model R² isn’t as good as you wanted. For improvement, your remove the intercept term, your model R² becomes 0.8 from 0.3. Is it possible? How?
329.	After analyzing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s true? Without losing any information, can you still build a better model?
330.	You are given a data set consisting of variables having more than 30% missing values? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?
331.	‘People who bought this, also bought…’ recommendations seen on amazon is a result of which algorithm?
332.	Which data visualisation libraries do you use? What are your thoughts on the best data visualisation tools?
333.	How would you implement a recommendation system for our company’s users?
334.	How can we use your machine learning skills to generate revenue?
335.	What are the last machine learning papers you’ve read?
336.	Do you have research experience in machine learning?
337.	What are your favorite use cases of machine learning models?
338.	How would you approach the “Netflix Prize” competition?
339.	Where do you usually source datasets?
340.	How do you think Google is training data for self-driving cars?
341.	What do you understand by Type I vs Type II error ?
342.	You are working on a classification problem. For validation purposes, you’ve randomly sampled the training data set into train and validation. You are confident that your model will work incredibly well on unseen data since your validation accuracy i
343.	State the universal approximation theorem? What is the technique used to prove that?
344.	Given the universal approximation theorem, why can't a MLP still reach a arbitrarily small positive error?
345.	What is the mathematical motivation of Deep Learning as opposed to standard Machine Learning techniques?
346.	In standard Machine Learning vs. Deep Learning, how is the order of number of samples related to the order of regions that can be recognized in the function space?
347.	What are the reasons for choosing a deep model as opposed to shallow model? (1. Number of regions O(2^k) vs O(k) where k is the number of training examples 2. # linear regions carved out in the function space depends exponentially on the depth. )
348.	How Deep Learning tackles the curse of dimensionality?(Other sources    www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning)
349.	How will you implement dropout during forward and backward pass?
350.	What do you do if Neural network training loss/testing loss stays constant? (ask if there could be an error in your code, going deeper, going simpler…)
351.	Why do RNNs have a tendency to suffer from exploding/vanishing gradient? How to prevent this? (Talk about LSTM cell which helps the gradient from vanishing, but make sure you know why it does so. Talk about gradient clipping, and discuss whether to clip the gradient element wise, or clip the norm of the gradient.)
352.	Do you know GAN, VAE, and memory augmented neural network? Can you talk about it?
353.	Does using full batch means that the convergence is always better given unlimited power? (Beautiful explanation by Alex Seewald: https://www.quora.com/Is-full-batch-gradient-descent-with-unlimited-computer-power-always-better-than-mini-batch-gradient-descent)
354.	What is the problem with sigmoid during backpropagation? (Very small, between 0.25 and zero.)
355.	Given a black box machine learning algorithm that you can’t modify, how could you improve its error? (you can transform the input for example.)
356.	How to find the best hyper parameters? (Random search, grid search, Bayesian search (and what it is?))
357.	What is transfer learning?
358.	Compare and contrast L1-loss vs. L2-loss and L1-regularization vs. L2-regularization.
359.	Can you state Tom Mitchell's definition of learning and discuss T, P and E?
360.	What can be different types of tasks encountered in Machine Learning?
361.	What are supervised, unsupervised, semi-supervised, self-supervised, multi-instance learning, and reinforcement learning?
362.	Loosely how can supervised learning be converted into unsupervised learning and vice-versa?
363.	Consider linear regression. What are T, P and E?
364.	Derive the normal equation for linear regression.
365.	What do you mean by affine transformation? Discuss affine vs. linear transformation.
366.	Discuss training error, test error, generalization error, overfitting, and underfitting.
367.	Compare representational capacity vs. effective capacity of a model.
368.	Discuss VC dimension.
369.	What are nonparametric models? What is nonparametric learning?
370.	What is an ideal model? What is Bayes error? What is/are the source(s) of Bayes error occur?
371.	What is the no free lunch theorem in connection to Machine Learning?
372.	What is regularization? Intuitively, what does regularization do during the optimization procedure? (expresses preferences to certain solutions, implicitly and explicitly)
373.	What is weight decay? What is it added?
374.	What is a hyperparameter? How do you choose which settings are going to be hyperparameters and which are going to be learnt? (either difficult to optimize or not appropriate to learn - learning model capacity by learning the degree of a polynomial or coefficient of the weight decay term always results in choosing the largest capacity until it overfits on the training set)
375.	Why is a validation set necessary?
376.	What are the different types of cross-validation? When do you use which one?
377.	What are point estimation and function estimation in the context of Machine Learning? What is the relation between them?
378.	What is the maximal likelihood of a parameter vector $theta$? Where does the log come from?
379.	Prove that for linear regression MSE can be derived from maximal likelihood by proper assumptions.
380.	Why is maximal likelihood the preferred estimator in ML? (consistency and efficiency)
381.	Under what conditions do the maximal likelihood estimator guarantee consistency?
382.	What is cross-entropy of loss? (trick question)
383.	What is the difference between an optimization problem and a Machine Learning problem?
384.	How can a learning problem be converted into an optimization problem?
385.	What is empirical risk minimization? Why the term empirical? Why do we rarely use it in the context of deep learning?
386.	Name some typical loss functions used for regression. Compare and contrast. (L2-loss, L1-loss, and Huber loss)
387.	What is the 0-1 loss function? Why can't the 0-1 loss function or classification error be used as a loss function for optimizing a deep neural network? (Non-convex, gradient is either 0 or undefined. 
388.	1.What’s the difference between a generative and discriminative model?
389.	When should you use classification over regression?
390.	What evaluation approaches would you work to gauge the effectiveness of a machine learning model?
391.	models are known to return high accuracy, but you are unfortunate. Where did you miss?
392.	When is Ridge regression favorable over Lasso regression?
393.	While working on a data set, how do you select important variables? Explain your methods.
394.	We know that one hot encoding increasing the dimensionality of a data set. But, label encoding doesn’t. How ?
395.	Explain machine learning to me like a 5 year old.
396.	Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?
397.	Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?
398.	When does regularization becomes necessary in Machine Learning?
399.	What are parametric models? Give an example?
400.	What are 3 data preprocessing techniques to handle outliers?
401.	What are 3 ways of reducing dimensionality?
402.	How much data should you allocate for your training, validation, and test sets?
403.	If you split your data into train/test splits, is it still possible to overfit your model?
404.	How can you choose a classifier based on training set size?
405.	Explain Latent Dirichlet Allocation (LDA)
406.	What are some key business metrics for (S-a-a-S startup | Retail bank | e-Commerce site)?
407.	How can you help our marketing team be more efficient?
408.	Differentiate between Data Science , Machine Learning and AI.((https://www.dezyre.com/article/100-data-science-interview-questions-and-answers-general-for-2018/184))
409.	Python or R – Which one would you prefer for text analytics?
410.	Which technique is used to predict categorical responses?
411.	What is Interpolation and Extrapolation?
412.	What is power analysis?
413.	What is the difference between Supervised Learning and Unsupervised Learning?
414.	Explain the use of Combinatorics in data science.
415.	Why is vectorization considered a powerful method for optimizing numerical code?
416.	What is the goal of A/B Testing?
417.	What are various steps involved in an analytics project?
418.	Can you use machine learning for time series analysis?
419.	What is the difference between Bayesian Estimate and Maximum Likelihood Estimation (MLE)?
420.	What is multicollinearity and how you can overcome it?
421.	What is the difference between squared error and absolute error?
422.	Differentiate between wide and tall data formats?
423.	How would you develop a model to identify plagiarism?
424.	You created a predictive model of a quantitative outcome variable using multiple regressions. What are the steps you would follow to validate the model?
425.	What do you understand by long and wide data formats?
426.	What is the importance of having a selection bias?
427.	What do you understand by Fuzzy merging ? Which language will you use to handle it?
428.	How can you deal with different types of seasonality in time series modelling?
429.	What makes a dataset gold standard?
430.	Can you write the formula to calculate R-square?
431.	Difference between Generative and Discriminative models.
432.	How will you assess the statistical significance of an insight whether it is a real insight or just by chance?
433.	How would you create a taxonomy to identify key customer trends in unstructured data?
434.	What do you understand by feature vectors?
435.	How do data management procedures like missing data handling make selection bias worse?
436.	How's EM done?
437.	How can you plot ROC curves for multiple classes. - There is something called as amacro-averaging of weights where PRE = (PRE1 + PRE2 + --- + PREk )/K, Text methods (latent, etc), he asked if I knew anything about these.
438.	What is the difference between inductive machine learning and deductive machine learning?
439.	How will you know which machine learning algorithm to choose for your classification problem?
440.	What are Bayesian Networks (BN) ?
441.	What is algorithm independent machine learning?
442.	What is classifier in machine learning?
443.	In what areas Pattern Recognition is used?
444.	What is Genetic Programming?
445.	What is Inductive Logic Programming in Machine Learning?
446.	What is inductive machine learning?
447.	What are the five popular algorithms of Machine Learning?
448.	What are the different Algorithm techniques in Machine Learning?
449.	List down various approaches for machine learning?
450.	What are the different methods for Sequential Supervised Learning?
451.	What is batch statistical learning?
452.	What is PAC Learning?
453.	What is sequence learning?
454.	What are two techniques of Machine Learning ?
455.	How to use labeled and unlabeled data?
456.	What if you don’t have any labeled data?
457.	What if your data set is skewed (e.g. 99.99 % positive and 0.01% negative labels)?
458.	How to make training faster?
459.	How to make predictions faster?
460.	Write the equation describing a dynamical system. Can you unfold it? Now, can you use this to describe a RNN? (include hidden, input, output, etc.)
461.	What determines the size of an unfolded graph?
462.	What are the advantages of an unfolded graph? (arbitrary sequence length, parameter sharing, and illustrate information flow during forward and backward pass)
463.	What does the output of the hidden layer of a RNN at any arbitrary time t represent?
464.	Are the output of hidden layers of RNNs lossless? If not, why?
465.	RNNs are used for various tasks. From a RNNs point of view, what tasks are more demanding than others?
466.	Discuss some examples of important design patterns of classical RNNs.
467.	Write the equations for a classical RNN where hidden layer has recurrence. How would you define the loss in this case? What problems you might face while training it? (Discuss runtime)
468.	What is backpropagation through time? (BPTT)
469.	Consider a RNN that has only output to hidden layer recurrence. What are its advantages or disadvantages compared to a RNNhaving only hidden to hidden recurrence?
470.	What is Teacher forcing? Compare and contrast with BPTT.
471.	What is the disadvantage of using a strict teacher forcing technique? How to solve this?

472.	Explain the vanishing/exploding gradient phenomenon for recurrent neural networks. (use scalar and vector input scenarios)
473.	Why don't we see the vanishing/exploding gradient phenomenon in feedforward networks? (weights are different in different layers - Random block intialization paper)
474.	What is the key difference in architecture of LSTMs/GRUs compared to traditional RNNs? (Additive update instead of multiplicative)
475.	What is the difference between LSTM and GRU?
476.	Explain Gradient Clipping.
477.	Adam and RMSProp adjust the size of gradients based on previously seen gradients. Do they inherently perform gradient clipping? If no, why?
478.	Discuss RNNs in the context of Bayesian Machine Learning.
479.	Can we do Batch Normalization in RNNs? If not, what is the alternative? (BNorm would need future data; Layer Norm)
480.	What is an Autoencoder? What does it "auto-encode"?
481.	What were Autoencoders traditionally used for? Why there has been a resurgence of Autoencoders for generative modeling?
482.	What is recirculation?
483.	What loss functions are used for Autoencoders?
484.	What is a linear autoencoder? Can it be optimal (lowest training reconstruction error)? If yes, under what conditions?
485.	What is the difference between Autoencoders and PCA (can also be used for reconstruction - https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com).
486.	What is the impact of the size of the hidden layer in Autoencoders?
487.	What is an undercomplete Autoencoder? Why is it typically used for?
488.	What is a linear Autoencoder? Discuss it's equivalence with PCA. (only valid for undercomplete) Which one is better in reconstruction?
489.	What problems might a nonlinear undercomplete Autoencoder face?
490.	What are overcomplete Autoencoders? What problems might they face? Does the scenario change for linear overcomplete autoencoders? (identity function)
491.	Discuss the importance of regularization in the context of Autoencoders.
492.	Why does generative autoencoders not require regularization?
493.	What are sparse autoencoders?
494.	What is a denoising autoencoder? What are its advantages? How does it solve the overcomplete problem?
495.	What is score matching? Discuss it's connections to DAEs.
496.	Are there any connections between Autoencoders and RBMs?
497.	What is manifold learning? How are denoising and contractive autoencoders equipped to do manifold learning?
498.	What is a contractive autoencoder? Discuss its advantages. How does it solve the overcomplete problem?
499.	Why is a contractive autoencoder named so? (intuitive and mathematical)
500.	What are the practical issues with CAEs? How to tackle them?
501.	What is a stacked autoencoder? What is a deep autoencoder? Compare and contrast.
502.	Compare the reconstruction quality of a deep autoencoder vs. PCA.
503.	What is predictive sparse decomposition?
504.	Discuss some applications of Autoencoders.
505.	What is representation learning? Why is it useful? (for a particular architecture, for other tasks, etc.)
506.	What is the relation between Representation Learning and Deep Learning?
507.	What is one-shot and zero-shot learning (Google's NMT)? Give examples.
508.	What trade offs does representation learning have to consider?
509.	What is greedy layer-wise unsupervised pretraining (GLUP)? Why greedy? Why layer-wise? Why unsupervised? Why pretraining?
510.	What were/are the purposes of the above technique? (deep learning problem and initialization)
511.	Why does unsupervised pretraining work?
512.	When does unsupervised training work? Under which circumstances?
513.	Why might unsupervised pretraining act as a regularizer?
514.	What is the disadvantage of unsupervised pretraining compared to other forms of unsupervised learning?
515.	How do you control the regularizing effect of unsupervised pre-training?
516.	How to select the hyperparameters of each stage of GLUP?
517.	What cross-validation technique would you use on a time series dataset?(Time series data )
518.	How would you handle an imbalanced dataset?(Classification Algo in various situations)
519.	Name an example where ensemble techniques might be useful.(Ensemble models)
520.	What’s the “kernel trick” and how is it useful?(SVM)((https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
521.	Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?
522.	What is convex hull ? (svm)
523.	What are the advantages and disadvantages of decision trees?(DT)
524.	What are the advantages and disadvantages of neural networks?(Deep Learning)
525.	Why are ensemble methods superior to individual models?(Ensemble Models)
526.	Explain bagging.(NLP)
527.	What are Recommender Systems?(Recommendation system)
528.	Why data cleaning plays a vital role in analysis?(EDA)
529.	Differentiate between univariate, bivariate and multivariate analysis.(EDA)
530.	What is Linear Regression?
531.	What is Collaborative filtering?(recommendation systems)
532.	Are expected value and mean value different?
533.	What are categorical variables?(classification algo)
534.	How can you iterate over a list and also retrieve element indices at the same time?(Python)
535.	During analysis, how do you treat missing values?(EDA)
536.	Write a function that takes in two sorted lists and outputs a sorted list that is their union. (Python)
537.	How are confidence intervals constructed and how will you interpret them?(Probability )
538.	How will you explain logistic regression to an economist, physican scientist and biologist?(Logistic regression)
539.	Is it better to have too many false negatives or too many false positives?(Performance measurement models)
540.	What do you understand by statistical power of sensitivity and how do you calculate it?(Probability)
541.	Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa.(SVM)
542.	Write a program in Python which takes input as the diameter of a coin and weight of the coin and produces output as the money value of the coin.(Programming)
543.	What are the basic assumptions to be made for linear regression?(Linear regression)
544.	Difference between convex and non-convex cost function; what does it mean when a cost function is non-convex? (SVM)
545.	Stochastic Gradient Descent: if it is faster, why don't we always use it?(Linear regression)
546.	Difference between SVM and Log R - Easy(SVM)
547.	Does SVM give any probabilistic output - I said no it doesn't and it was wrong! He gave me hints but I couldn't figure it out!(SVM)
548.	What are the support vectors in SVM
549.	Mention the difference between Data Mining and Machine learning?(General)
550.	You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?(EDA)
551.	Why is Naïve Bayes machine learning algorithm naïve?(Naive Bayes)
552.	Explain prior probability, likelihood and marginal likelihood in context of naïve Bayes algorithm?
553.	What are the three stages to build the hypotheses or model in machine learning?
554.	What is the standard approach to supervised learning?
555.	What is ‘Training set’ and ‘Test set’?
556.	List down various approaches for machine learning?
557.	How to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?
558.	Name some feature extraction techniques used for dimensionality reduction.
559.	List some use cases where classification machine learning algorithms can be used.
560.	What kind of problems does regularization solve?
561.	How much data will you allocate for your training, validation and test sets?
562.	Which one would you prefer to choose – model accuracy or model performance?
563.	Describe some popular machine learning methods.
564.	What is not Machine Learning?
565.	Explain what is the function of ‘Unsupervised Learning’?
566.	How will you differentiate between supervised and unsupervised learning? Give few examples of algorithms for supervised learning?
567.	What is linear regression? Why is it called linear?
568.	How does the variance of the error term change with the number of predictors, in OLS?
569.	Do we always need the intercept term? When do we need it and when do we not?
570.	How interpretable is the given machine learning model?
571.	What will you do if training results in very low accuracy?
572.	Does the developed machine learning model have convergence problems?
573.	Which tools and environments have you used to train and assess machine learning models?
574.	How will you apply machine learning to images?
575.	What is collinearity and what to do with it?
576.	How to remove multicollinearity?
577.	What is overfitting a regression model? What are ways to avoid it?
578.	What is loss function in a Neural Network?
579.	Explain the difference between MLE and MAP inference.
580.	What is boosting?
581.	If the gradient descent does not converge, what could be the problem?
582.	How will you check for a valid binary search tree?
583.	How to check if the regression model fits the data well?
584.	What are parametric models?()
585.	What’s the trade-off between bias and variance?
586.	Explain how a ROC curve works.(Performance Measurement Models)
587.	What is the Box-Cox transformation used for?(Probability)
588.	Define precision and recall.(Performance measurement models?
589.	what is the function of ‘Unsupervised Learning’?(Unsuperwised learning)
590.	What is Perceptron in Machine Learning?(Deep Learning)
591.	What is ensemble learning?(Ensemble Models)
592.	What are the two paradigms of ensemble methods?(Ensemble Models)
593.	What is PCA, KPCA and ICA used for?
594.	You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?    www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
595.	You are given a data set consisting of variables having more than 30% missing values? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?    www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
596.	Compare "Frequentist probability" vs. "Bayesian probability"?
597.	What is a random variable?
598.	What is a joint probability distribution?
599.	What are the conditions for a function to be a probability mass function?
600.	What are the conditions for a function to be a probability density function 
601.	What is a marginal probability? Given the joint probability function, how will you calculate it?
602.	What is conditional probability? Given the joint probability function, how will you calculate it?
603.	State the Chain rule of conditional probabilities.
604.	What are the conditions for independence and conditional independence of two random variables?
605.	What are expectation, variance and covariance?
606.	Compare covariance and independence.
607.	What is the covariance for a vector of random variables?
608.	What is a Bernoulli distribution? Calculate the expectation and variance of a random variable that follows Bernoulli distribution?
609.	What is a multinoulli distribution?
610.	What is a normal distribution?
611.	Why is the normal distribution a default choice for a prior over a set of real numbers?
612.	What is the central limit theorem?
613.	What are exponential and Laplace distribution?
614.	What are Dirac distribution and Empirical distribution?
615.	What is mixture of distributions?
616.	Name two common examples of mixture of distributions? (Empirical and Gaussian Mixture)
617.	Is Gaussian mixture model a universal approximator of densities?
618.	Write the formula for logistic and softplus function.
619.	Write the formula for Bayes rule.
620.	What do you mean by measure zero and almost everywhere?
621.	If two random variables are related in a deterministic way, how are the PDFs related?
622.	Define self-information. What are its units?
623.	What are Shannon entropy and differential entropy?
624.	What is Kullback-Leibler (KL) divergence?
625.	Can KL divergence be used as a distance measure?
626.	Define cross-entropy.
627.	What are structured probabilistic models or graphical models?
628.	In the context of structured probabilistic models, what are directed and undirected models? How are they represented? What are cliques in undirected structured probabilistic models?
629.	What is Bayes’ Theorem? How is it useful in a machine learning context?
630.	Why is “Naive” Bayes naive?
631.	What’s a Fourier transform?
632.	What’s the difference between probability and likelihood?
633.	Explain prior probability, likelihood and marginal likelihood in context of naive Bayes algorithm?
634.	What is the difference between covariance and correlation?
635.	Is it possible capture the correlation between continuous and categorical variable? If yes, how?
636.	What is the Box-Cox transformation used for?
637.	What do you understand by the term Normal Distribution?
638.	What does P-value signify about the statistical data?
639.	A test has a true positive rate of 100% and false positive rate of 5%. There is a population with a 1/1000 rate of having the condition the test identifies. Considering a positive test, what is the probability of having that condition?
640.	How you can make data normal using Box-Cox transformation?
641.	Explain about the box cox transformation in regression models.
642.	What is the difference between skewed and uniform distribution?
643.	What do you understand by Hypothesis in the content of Machine Learning?
644.	How will you find the correlation between a categorical variable and a continuous variable ?
645.	What does LogR give ? I said Posterior probability (P(y|x=0 or x=1))
646.	Evaluation of LogR -
647.	How are the params updated - I was able to answer with formulae!
648.	When doing an EM for GMM, how do you find the mixture weights ? I replied that for 2 Gaussians, the prior or the mixture weight can be assumed to be a Bernoulli distribution.
649.	If x ~ N(0,1), what does 2x follow
650.	How would you sample for a GMM
651.	How to sample from a Normal Distribution with known mean and variance.
652.	In experimental design, is it necessary to do randomization? If yes, why
653.	How do you handle missing or corrupted data in a dataset?
654.	Do you have experience with Spark or big data tools for machine learning?
655.	In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbours. Why not manhattan distance ?    www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/)
656.	How to test and know whether or not we have overfitting problem?   
657.	How is kNN different from k-means clustering?    stats.stackexchange.com/questions/56500/what-are-the-main-differences-between-k-means-and-k-nearest-neighbours)
658.	Can you explain the difference between a Test Set and a Validation Set?    stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo)
659.	How can you avoid overfitting in KNN?
660.	Which is more important to you– model accuracy, or model performance?
661.	Can you cite some examples where a false positive is important than a false negative?
662.	Can you cite some examples where a false negative important than a false positive?
663.	Can you cite some examples where both false positive and false negatives are equally important?
664.	What is the most frequent metric to assess model accuracy for classification problems?
665.	Why is Area Under ROC Curve (AUROC) better than raw accuracy as an out-of- sample evaluation metric?
666.	Define Similarity or Distance matrix.?
667.	Time complexity of Naive Bayes algo  Best and worst cases?
668.	What are the differences between “Bayesian” and “Frequentist” approach for Machine Learning?
669.	Compare and contrast maximum likelihood and maximum a posteriori estimation.
670.	How does Bayesian methods do automatic feature selection?
671.	What do you mean by Bayesian regularization?
672.	When will you use Bayesian methods instead of Frequentist methods? (Small dataset, large feature set)
673.	After analysing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s true? Without losing any information, can you still build a better model?    google-interview-hacks.blogspot.in/2017/04/after-analyzing-model-your-manager-has.html)
674.	What are the basic assumptions to be made for linear regression?(Refer:https://www.statisticssolutions.com/assumptions-of-linear-regression/)
675.	What is the difference between stochastic gradient descent (SGD) and gradient descent (GD)?    stats.stackexchange.com/questions/317675/gradient-descent-gd-vs-stochastic-gradient-descent-sgd)
676.	When would you use GD over SDG, and vice-versa?    elitedatascience.com/machine-learning-interview-questions-answers)
677.	How do you decide whether your linear regression model fits the data?    www.researchgate.net/post/What_statistical_test_is_required_to_assess_goodness_of_fit_of_a_linear_or_nonlinear_regression_equation)
678.	Is it possible to perform logistic regression with Microsoft Excel?    www.youtube.com/watch?v=EKRjDurXau0)
679.	When will you use classification over regression?    www.quora.com/When-will-you-use-classification-over-regression)
680.	Why isn't Logistic Regression called Logistic Classification?(Refer :https://stats.stackexchange.com/questions/127042/why-isnt-logistic-regression-called-logistic-classification/127044)
681.	Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa.    datascience.stackexchange.com/questions/6838/when-to-use-random-forest-over-svm-and-vice-versa)
682.	What is convex hull ?    en.wikipedia.org/wiki/Convex_hull)
683.	What is a large margin classifier?
684.	Why SVM is an example of a large margin classifier?
685.	SVM being a large margin classifier, is it influenced by outliers? (Yes, if C is large, otherwise not)
686.	What is the role of C in SVM?
687.	In SVM, what is the angle between the decision boundary and theta?
688.	What is the mathematical intuition of a large margin classifier?
689.	What is a kernel in SVM? Why do we use kernels in SVM?
690.	What is a similarity function in SVM? Why it is named so?
691.	How are the landmarks initially chosen in an SVM? How many and where?
692.	Can we apply the kernel trick to logistic regression? Why is it not used in practice then?
693.	What is the difference between logistic regression and SVM without a kernel? (Only in implementation – one is much more efficient and has good optimization packages)
694.	How does the SVM parameter C affect the bias/variance trade off? (Remember C = 1/lambda; lambda increases means variance decreases)
695.	How does the SVM kernel parameter sigma^2 affect the bias/variance trade off?
696.	Can any similarity function be used for SVM? (No, have to satisfy Mercer’s theorem)
697.	Logistic regression vs. SVMs: When to use which one? ( Let's say n and m are the number of features and training samples respectively. If n is large relative to m use log. Reg. or SVM with linear kernel, If n is small and m is intermediate, SVM with Gaussian kernel, If n is small and m is massive, Create or add more features then use log. Reg. or SVM without a kernel)
698.	What is the difference between supervised and unsupervised machine learning?
699.	You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?(Refer :https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
700.	Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?(Refer:https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
701.	You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven’t you trained your model perfectly?(Refer : https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
702.	How would you implement a recommendation system for our company’s users?    www.infoworld.com/article/3241852/machine-learning/how-to-implement-a-recommender-system.html)
703.	How would you approach the “Netflix Prize” competition?(Refer http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/)
704.	‘People who bought this, also bought…’ recommendations seen on amazon is a result of which algorithm?(Please refer Apparel recommendation system case study,Refer:https://measuringu.com/affinity-analysis/)
705.	Pick an algorithm. Write the psuedo-code for a parallel implementation.
706.	What are some differences between a linked list and an array?(Programming)
707.	Describe a hash table.
708.	What is sampled softmax?
709.	Why is it difficult to train a RNN with SGD?
710.	How do you tackle the problem of exploding gradients? (By gradient clipping)
711.	What is the problem of vanishing gradients? (RNN doesn't tend to remember much things from the past)
712.	How do you tackle the problem of vanishing gradients? (By using LSTM)
713.	Explain the memory cell of a LSTM. (LSTM allows forgetting of data and using long memory when appropriate.)
714.	What type of regularization do one use in LSTM?
715.	What is Beam Search?
716.	How to automatically caption an image? (CNN + LSTM)
717.	What is the mathematical motivation of Deep Learning as opposed to standard Machine Learning techniques?
718.	In standard Machine Learning vs. Deep Learning, how is the order of number of samples related to the order of regions that can be recognized in the function space?
719.	What are the reasons for choosing a deep model as opposed to shallow model? (1. Number of regions O(2^k) vs O(k) where k is the number of training examples 2. # linear regions carved out in the function space depends exponentially on the depth. )
720.	How Deep Learning tackles the curse of dimensionality?
721.	Why do RNNs have a tendency to suffer from exploding/vanishing gradient? How to prevent this? (Talk about LSTM cell which helps the gradient from vanishing, but make sure you know why it does so. Talk about gradient clipping, and discuss whether to clip the gradient element wise, or clip the norm of the gradient.)
722.	What is the problem with sigmoid during backpropagation? (Very small, between 0.25 and zero.)
723.	What is transfer learning?
724.	Write the equation describing a dynamical system. Can you unfold it? Now, can you use this to describe a RNN? (include hidden, input, output, etc.)
725.	What determines the size of an unfolded graph?
726.	What are the advantages of an unfolded graph? (arbitrary sequence length, parameter sharing, and illustrate information flow during forward and backward pass)
727.	What does the output of the hidden layer of a RNN at any arbitrary time t represent?
728.	Are the output of hidden layers of RNNs lossless? If not, why?
729.	RNNs are used for various tasks. From a RNNs point of view, what tasks are more demanding than others?
730.	Discuss some examples of important design patterns of classical RNNs.
731.	Write the equations for a classical RNN where hidden layer has recurrence. How would you define the loss in this case? What problems you might face while training it? (Discuss runtime)
732.	What is backpropagation through time? (BPTT)
733.	Consider a RNN that has only output to hidden layer recurrence. What are its advantages or disadvantages compared to a RNNhaving only hidden to hidden recurrence?
734.	What is Teacher forcing? Compare and contrast with BPTT.
735.	What is the disadvantage of using a strict teacher forcing technique? How to solve this?
736.	Explain the vanishing/exploding gradient phenomenon for recurrent neural networks. (use scalar and vector input scenarios)
737.	Why don't we see the vanishing/exploding gradient phenomenon in feedforward networks? (weights are different in different layers - Random block intialization paper)
738.	What is the key difference in architecture of LSTMs/GRUs compared to traditional RNNs? (Additive update instead of multiplicative)
739.	What is the difference between LSTM and GRU?
740.	Explain Gradient Clipping.
741.	Adam and RMSProp adjust the size of gradients based on previously seen gradients. Do they inherently perform gradient clipping? If no, why?
742.	Discuss RNNs in the context of Bayesian Machine Learning.
743.	Can we do Batch Normalization in RNNs? If not, what is the alternative? (BNorm would need future data; Layer Norm)
744.	What is representation learning? Why is it useful? (for a particular architecture, for other tasks, etc.)
745.	What is the relation between Representation Learning and Deep Learning?
746.	What is one-shot and zero-shot learning (Google's NMT)? Give examples.
747.	What trade offs does representation learning have to consider?
748.	What is greedy layer-wise unsupervised pretraining (GLUP)? Why greedy? Why layer-wise? Why unsupervised? Why pretraining?
749.	What were/are the purposes of the above technique? (deep learning problem and initialization)
750.	Why does unsupervised pretraining work?
751.	When does unsupervised training work? Under which circumstances?
752.	Why might unsupervised pretraining act as a regularizer?
753.	What is the disadvantage of unsupervised pretraining compared to other forms of unsupervised learning?
754.	How do you control the regularizing effect of unsupervised pre-training?
755.	How to select the hyperparameters of each stage of GLUP?

756.	 
757.	References:   
758.	https://medium.com/acing-ai/salesforce-ai-interview-questions-acing-the-ai-interview-75e177c4734 
759.	https://medium.com/acing-ai/microsoft-ai-interview-questions-acing-the-ai-interview-be6972f790ea  
760.	https://medium.com/acing-ai/apple-ai-interview-questions-acing-the-ai-interview-803a65b0e795 
761.	https://medium.com/acing-ai/amazon-ai-interview-questions-acing-the-ai-interview-3ed4e671920f 
762.	https://medium.com/acing-ai/uber-ai-interview-questions-acing-the-ai-interview-9532794bc057 
763.	https://medium.com/acing-ai/steps-to-ace-the-ai-interview-part-1-298249080e59 
764.	https://medium.com/acing-ai/steps-to-ace-the-ai-interview-part-2-b25f91582f5f 
765.	https://medium.com/acing-ai/google-ai-interview-questions-acing-the-ai-interview-1791ad7dc3ae 
766.	https://medium.com/acing-ai/facebook-ai-interview-questions-acing-the-ai-interview-5982add0af55 
767.	https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/ 
768.	https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/ 
769.	https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/ 
770.	https://www.listendata.com/2017/03/predictive-modeling-interview-questions.html 
771.	https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/
772.	https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/ 
773.	https://medium.com/acing-ai/adobe-ai-interview-questions-acing-the-ai-interview-ef7a8099110b 
774.	https://www.listendata.com/2018/03/regression-analysis.html 
775.	https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/ 
776.	https://vitalflux.com/decision-tree-algorithm-concepts-interview-question 
777.	https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-tree-based-models/ 
778.	https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/ 
779.	https://www.kdnuggets.com/tag/interview-questions

780.	LinkedIn posts:
781.	While preparing for job interviews I found some great resources on Machine Learning System designs from Facebook, Twitter, Google, Airbnb, Uber, Instagram and Netflix. 
782.	I find this to be a fascinating topic because it's something not often covered in online courses. 
783.	Twitter Newsfeed: https://bit.ly/37Vg8tN Twitter Ads: https://bit.ly/3bXnS1o Instagram Newsfeed: https://bit.ly/2ur0ZCJ Instagram Explore: https://bit.ly/37X53bv Facebook Deep Entity Classification: https://bit.ly/2upfbfl Uber Eats Recommendations: https://ubr.to/2ur1c8Z https://ubr.to/2HTMiv8 https://ubr.to/2SSL0Xw Uber Maps: https://ubr.to/3a0dynv Airbnb: https://bit.ly/2VhDuaf https://bit.ly/39Z63gx https://bit.ly/2SSgZqC https://bit.ly/2v0UQxr https://bit.ly/2urR9Az https://bit.ly/38VWH5j Airbnb Experiences: https://bit.ly/2vbNjf9 Google: https://bit.ly/2SYi9Bb Linkedin: https://lnkd.in/dH39hsG Netflix Recommendations: https://lnkd.in/dW3EU2p Spotify Recommendations: https://bit.ly/38RpWpM
784.	 
785.	General Interview Questions:
786.	Tell me about yourself
787.	 
788.	Github:

789.	General questions
790.	How will you implement dropout during forward and backward pass?
791.	What do you do if Neural network training loss/testing loss stays constant? (ask if there could be an error in your code, going deeper, going simpler…)
792.	Why do RNNs have a tendency to suffer from exploding/vanishing gradient? How to prevent this? (Talk about LSTM cell which helps the gradient from vanishing, but make sure you know why it does so. Talk about gradient clipping, and discuss whether to clip the gradient element wise, or clip the norm of the gradient.)
793.	Do you know GAN, VAE, and memory augmented neural network? Can you talk about it?
794.	Does using full batch means that the convergence is always better given unlimited power? (Beautiful explanation by Alex Seewald: https://www.quora.com/Is-full-batch-gradient-descent-with-unlimited-computer-power-always-better-than-mini-batch-gradient-descent)
795.	What is the problem with sigmoid during backpropagation? (Very small, between 0.25 and zero.)
796.	Given a black box machine learning algorithm that you can’t modify, how could you improve its error? (you can transform the input for example.)
797.	How to find the best hyper parameters? (Random search, grid search, Bayesian search (and what it is?))
798.	What is transfer learning?
799.	Compare and contrast L1-loss vs. L2-loss and L1-regularization vs. L2-regularization.
800.	Machine Learning basics
801.	Can you state Tom Mitchell's definition of learning and discuss T, P and E?
802.	What can be different types of tasks encountered in Machine Learning?
803.	What are supervised, unsupervised, semi-supervised, self-supervised, multi-instance learning, and reinforcement learning?
804.	Loosely how can supervised learning be converted into unsupervised learning and vice-versa?
805.	Consider linear regression. What are T, P and E?
806.	Derive the normal equation for linear regression.
807.	What do you mean by affine transformation? Discuss affine vs. linear transformation.
808.	Discuss training error, test error, generalization error, overfitting, and underfitting.
809.	Compare representational capacity vs. effective capacity of a model.
810.	Discuss VC dimension.
811.	What are nonparametric models? What is nonparametric learning?
812.	What is an ideal model? What is Bayes error? What is/are the source(s) of Bayes error occur?
813.	What is the no free lunch theorem in connection to Machine Learning?
814.	What is regularization? Intuitively, what does regularization do during the optimization procedure? (expresses preferences to certain solutions, implicitly and explicitly)
815.	What is weight decay? What is it added?
816.	What is a hyperparameter? How do you choose which settings are going to be hyperparameters and which are going to be learnt? (either difficult to optimize or not appropriate to learn - learning model capacity by learning the degree of a polynomial or coefficient of the weight decay term always results in choosing the largest capacity until it overfits on the training set)
817.	Why is a validation set necessary?
818.	What are the different types of cross-validation? When do you use which one?
819.	What are point estimation and function estimation in the context of Machine Learning? What is the relation between them?
820.	What is the maximal likelihood of a parameter vector $theta$? Where does the log come from?
821.	Prove that for linear regression MSE can be derived from maximal likelihood by proper assumptions.
822.	Why is maximal likelihood the preferred estimator in ML? (consistency and efficiency)
823.	Under what conditions do the maximal likelihood estimator guarantee consistency?
824.	What is cross-entropy of loss? (trick question)
825.	Optimization procedures
826.	What is the difference between an optimization problem and a Machine Learning problem?
827.	How can a learning problem be converted into an optimization problem?
828.	What is empirical risk minimization? Why the term empirical? Why do we rarely use it in the context of deep learning?
829.	Name some typical loss functions used for regression. Compare and contrast. (L2-loss, L1-loss, and Huber loss)
830.	What is the 0-1 loss function? Why can't the 0-1 loss function or classification error be used as a loss function for optimizing a deep neural network? (Non-convex, gradient is either 0 or undefined. https://davidrosenberg.github.io/ml2015/docs/3a.loss-functions.pdf)
831.	Parameter initialization
832.	Sequence Modeling
833.	Write the equation describing a dynamical system. Can you unfold it? Now, can you use this to describe a RNN? (include hidden, input, output, etc.)
834.	What determines the size of an unfolded graph?
835.	What are the advantages of an unfolded graph? (arbitrary sequence length, parameter sharing, and illustrate information flow during forward and backward pass)
836.	What does the output of the hidden layer of a RNN at any arbitrary time t represent?
837.	Are the output of hidden layers of RNNs lossless? If not, why?
838.	RNNs are used for various tasks. From a RNNs point of view, what tasks are more demanding than others?
839.	Discuss some examples of important design patterns of classical RNNs.
840.	Write the equations for a classical RNN where hidden layer has recurrence. How would you define the loss in this case? What problems you might face while training it? (Discuss runtime)
841.	What is backpropagation through time? (BPTT)
842.	Consider a RNN that has only output to hidden layer recurrence. What are its advantages or disadvantages compared to a RNNhaving only hidden to hidden recurrence?
843.	What is Teacher forcing? Compare and contrast with BPTT.
844.	What is the disadvantage of using a strict teacher forcing technique? How to solve this?
845.	Explain the vanishing/exploding gradient phenomenon for recurrent neural networks. (use scalar and vector input scenarios)
846.	Why don't we see the vanishing/exploding gradient phenomenon in feedforward networks? (weights are different in different layers - Random block intialization paper)
847.	What is the key difference in architecture of LSTMs/GRUs compared to traditional RNNs? (Additive update instead of multiplicative)
848.	What is the difference between LSTM and GRU?
849.	Explain Gradient Clipping.
850.	Adam and RMSProp adjust the size of gradients based on previously seen gradients. Do they inherently perform gradient clipping? If no, why?
851.	Discuss RNNs in the context of Bayesian Machine Learning.
852.	Can we do Batch Normalization in RNNs? If not, what is the alternative? (BNorm would need future data; Layer Norm)
853.	Autoencoders
854.	What is an Autoencoder? What does it "auto-encode"?
855.	What were Autoencoders traditionally used for? Why there has been a resurgence of Autoencoders for generative modeling?
856.	What is recirculation?
857.	What loss functions are used for Autoencoders?
858.	What is a linear autoencoder? Can it be optimal (lowest training reconstruction error)? If yes, under what conditions?
859.	What is the difference between Autoencoders and PCA (can also be used for reconstruction - https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com).
860.	What is the impact of the size of the hidden layer in Autoencoders?
861.	What is an undercomplete Autoencoder? Why is it typically used for?
862.	What is a linear Autoencoder? Discuss it's equivalence with PCA. (only valid for undercomplete) Which one is better in reconstruction?
863.	What problems might a nonlinear undercomplete Autoencoder face?
864.	What are overcomplete Autoencoders? What problems might they face? Does the scenario change for linear overcomplete autoencoders? (identity function)
865.	Discuss the importance of regularization in the context of Autoencoders.
866.	Why does generative autoencoders not require regularization?
867.	What are sparse autoencoders?
868.	What is a denoising autoencoder? What are its advantages? How does it solve the overcomplete problem?
869.	What is score matching? Discuss it's connections to DAEs.
870.	Are there any connections between Autoencoders and RBMs?
871.	What is manifold learning? How are denoising and contractive autoencoders equipped to do manifold learning?
872.	What is a contractive autoencoder? Discuss its advantages. How does it solve the overcomplete problem?
873.	Why is a contractive autoencoder named so? (intuitive and mathematical)
874.	What are the practical issues with CAEs? How to tackle them?
875.	What is a stacked autoencoder? What is a deep autoencoder? Compare and contrast.
876.	Compare the reconstruction quality of a deep autoencoder vs. PCA.
877.	What is predictive sparse decomposition?
878.	Discuss some applications of Autoencoders.
879.	Representation Learning
880.	What is representation learning? Why is it useful? (for a particular architecture, for other tasks, etc.)
881.	What is the relation between Representation Learning and Deep Learning?
882.	What is one-shot and zero-shot learning (Google's NMT)? Give examples.
883.	What trade offs does representation learning have to consider?
884.	What is greedy layer-wise unsupervised pretraining (GLUP)? Why greedy? Why layer-wise? Why unsupervised? Why pretraining?
885.	What were/are the purposes of the above technique? (deep learning problem and initialization)
886.	Why does unsupervised pretraining work?
887.	When does unsupervised training work? Under which circumstances?
888.	Why might unsupervised pretraining act as a regularizer?
889.	What is the disadvantage of unsupervised pretraining compared to other forms of unsupervised learning?
890.	How do you control the regularizing effect of unsupervised pretraining?
891.	How to select the hyperparameters of each stage of GLUP?
892.	Monte Carlo Methods
893.	What are deterministic algorithms? (nothing random)
894.	What are Las vegas algorithms? (exact or no solution, random resources)
895.	What are deterministic approximate algorithms? (solution is not exact but the error is known)
896.	What are Monte Carlo algorithms? (approximate solution with random error)
897.	Adversarial Networks
898.	Discuss state-of-the-art attack and defense techniques for adversarial models.
899.	 
900.	Learning Theory
901.	Describe bias and variance with examples.
902.	What is Empirical Risk Minimization?
903.	What is Union bound and Hoeffding's inequality?
904.	Write the formulae for training error and generalization error. Point out the differences.
905.	State the uniform convergence theorem and derive it.
906.	What is sample complexity bound of uniform convergence theorem?
907.	What is error bound of uniform convergence theorem?
908.	What is the bias-variance trade-off theorem?
909.	From the bias-variance trade-off, can you derive the bound on training set size?
910.	What is the VC dimension?
911.	What does the training set size depend on for a finite and infinite hypothesis set? Compare and contrast.
912.	What is the VC dimension for an n-dimensional linear classifier?
913.	How is the VC dimension of a SVM bounded although it is projected to an infinite dimension?
914.	Considering that Empirical Risk Minimization is a NP-hard problem, how does logistic regression and SVM loss work?
915.	Model and feature selection
916.	Why are model selection methods needed?
917.	How do you do a trade-off between bias and variance?
918.	What are the different attributes that can be selected by model selection methods?
919.	Why is cross-validation required?
920.	Describe different cross-validation techniques.
921.	What is hold-out cross validation? What are its advantages and disadvantages?
922.	What is k-fold cross validation? What are its advantages and disadvantages?
923.	What is leave-one-out cross validation? What are its advantages and disadvantages?
924.	Why is feature selection required?
925.	Describe some feature selection methods.
926.	What is forward feature selection method? What are its advantages and disadvantages?
927.	What is backward feature selection method? What are its advantages and disadvantages?
928.	What is filter feature selection method and describe two of them?
929.	What is mutual information and KL divergence?
930.	Describe KL divergence intuitively.
931.	Curse of dimensionality
932.	Describe the curse of dimensionality with examples.
933.	What is local constancy or smoothness prior or regularization?
934.	Universal approximation of neural networks
935.	State the universal approximation theorem? What is the technique used to prove that?
936.	What is a Borel measurable function?
937.	Given the universal approximation theorem, why can't a MLP still reach a arbitrarily small positive error?
938.	Deep Learning motivation
939.	What is the mathematical motivation of Deep Learning as opposed to standard Machine Learning techniques?
940.	In standard Machine Learning vs. Deep Learning, how is the order of number of samples related to the order of regions that can be recognized in the function space?
941.	What are the reasons for choosing a deep model as opposed to shallow model? (1. Number of regions O(2^k) vs O(k) where k is the number of training examples 2. # linear regions carved out in the function space depends exponentially on the depth. )
942.	How Deep Learning tackles the curse of dimensionality?
943.	Support Vector Machine
944.	How can the SVM optimization function be derived from the logistic regression optimization function?
945.	What is a large margin classifier?
946.	Why SVM is an example of a large margin classifier?
947.	SVM being a large margin classifier, is it influenced by outliers? (Yes, if C is large, otherwise not)
948.	What is the role of C in SVM?
949.	In SVM, what is the angle between the decision boundary and theta?
950.	What is the mathematical intuition of a large margin classifier?
951.	What is a kernel in SVM? Why do we use kernels in SVM?
952.	What is a similarity function in SVM? Why it is named so?
953.	How are the landmarks initially chosen in an SVM? How many and where?
954.	Can we apply the kernel trick to logistic regression? Why is it not used in practice then?
955.	What is the difference between logistic regression and SVM without a kernel? (Only in implementation – one is much more efficient and has good optimization packages)
956.	How does the SVM parameter C affect the bias/variance trade off? (Remember C = 1/lambda; lambda increases means variance decreases)
957.	How does the SVM kernel parameter sigma^2 affect the bias/variance trade off?
958.	Can any similarity function be used for SVM? (No, have to satisfy Mercer’s theorem)
959.	Logistic regression vs. SVMs: When to use which one? ( Let's say n and m are the number of features and training samples respectively. If n is large relative to m use log. Reg. or SVM with linear kernel, If n is small and m is intermediate, SVM with Gaussian kernel, If n is small and m is massive, Create or add more fetaures then use log. Reg. or SVM without a kernel)
960.	Bayesian Machine Learning
961.	What are the differences between “Bayesian” and “Frequentist” approach for Machine Learning?
962.	Compare and contrast maximum likelihood and maximum a posteriori estimation.
963.	How does Bayesian methods do automatic feature selection?
964.	What do you mean by Bayesian regularization?
965.	When will you use Bayesian methods instead of Frequentist methods? (Small dataset, large feature set)
966.	Regularization
967.	What is L1 regularization?
968.	What is L2 regularization?
969.	Compare L1 and L2 regularization.
970.	Why does L1 regularization result in sparse models? here
971.	Evaluation of Machine Learning systems
972.	What are accuracy, sensitivity, specificity, ROC?
973.	What are precision and recall?
974.	Describe t-test in the context of Machine Learning.
975.	Clustering
976.	Describe the k-means algorithm.
977.	What is distortion function? Is it convex or non-convex?
978.	Tell me about the convergence of the distortion function.
979.	Topic: EM algorithm
980.	What is the Gaussian Mixture Model?
981.	Describe the EM algorithm intuitively.
982.	What are the two steps of the EM algorithm
983.	Compare GMM vs GDA.
984.	Dimensionality Reduction
985.	Why do we need dimensionality reduction techniques? (data compression, speeds up learning algorithm and visualizing data)
986.	What do we need PCA and what does it do? (PCA tries to find a lower dimensional surface such the sum of the squared projection error is minimized)
987.	What is the difference between logistic regression and PCA?
988.	What are the two pre-processing steps that should be applied before doing PCA? (mean normalization and feature scaling)
989.	Basics of Natural Language Processing
990.	What is WORD2VEC?
991.	What is t-SNE? Why do we use PCA instead of t-SNE?
992.	What is sampled softmax?
993.	Why is it difficult to train a RNN with SGD?
994.	How do you tackle the problem of exploding gradients? (By gradient clipping)
995.	What is the problem of vanishing gradients? (RNN doesn't tend to remember much things from the past)
996.	How do you tackle the problem of vanishing gradients? (By using LSTM)
997.	Explain the memory cell of a LSTM. (LSTM allows forgetting of data and using long memory when appropriate.)
998.	What type of regularization do one use in LSTM?
999.	What is Beam Search?
1000.	How to automatically caption an image? (CNN + LSTM)
1001.	Miscellaneous
1002.	What is the difference between loss function, cost function and objective function?
1003.	 
1004.	Linear Algebra
1005.	What is broadcasting in connection to Linear Algebra?
1006.	What are scalars, vectors, matrices, and tensors?
1007.	What is Hadamard product of two matrices?
1008.	What is an inverse matrix?
1009.	If inverse of a matrix exists, how to calculate it?
1010.	What is the determinant of a square matrix? How is it calculated (Laplace expansion)? What is the connection of determinant to eigenvalues?
1011.	Discuss span and linear dependence.
1012.	What is Ax = b? When does Ax =b has a unique solution?
1013.	In Ax = b, what happens when A is fat or tall?
1014.	When does inverse of A exist?
1015.	What is a norm? What is L1, L2 and L infinity norm?
1016.	What are the conditions a norm has to satisfy?
1017.	Why is squared of L2 norm preferred in ML than just L2 norm?
1018.	When L1 norm is preferred over L2 norm?
1019.	Can the number of nonzero elements in a vector be defined as L0 norm? If no, why?
1020.	What is Frobenius norm?
1021.	What is a diagonal matrix? (D_i,j = 0 for i != 0)
1022.	Why is multiplication by diagonal matrix computationally cheap? How is the multiplication different for square vs. non-square diagonal matrix?
1023.	At what conditions does the inverse of a diagonal matrix exist? (square and all diagonal elements non-zero)
1024.	What is a symmetrix matrix? (same as its transpose)
1025.	What is a unit vector?
1026.	When are two vectors x and y orthogonal? (x.T * y = 0)
1027.	At R^n what is the maximum possible number of orthogonal vectors with non-zero norm?
1028.	When are two vectors x and y orthonormal? (x.T * y = 0 and both have unit norm)
1029.	What is an orthogonal matrix? Why is computationally preferred? (a square matrix whose rows are mutually orthonormal and columns are mutually orthonormal.)
1030.	What is eigendecomposition, eigenvectors and eigenvalues?
1031.	How to find eigen values of a matrix?
1032.	Write the eigendecomposition formula for a matrix. If the matrix is real symmetric, how will this change?
1033.	Is the eigendecomposition guaranteed to be unique? If not, then how do we represent it?
1034.	What are positive definite, negative definite, positive semi definite and negative semi definite matrices?
1035.	What is SVD? Why do we use it? Why not just use ED?
1036.	Given a matrix A, how will you calculate its SVD?
1037.	What are singular values, left singulars and right singulars?
1038.	What is the connection of SVD of A with functions of A?
1039.	Why are singular values always non-negative?
1040.	What is the Moore Penrose pseudo inverse and how to calculate it?
1041.	If we do Moore Penrose pseudo inverse on Ax = b, what solution is provided is A is fat? Moreover, what solution is provided if A is tall?
1042.	Which matrices can be decomposed by ED? (Any NxN square matrix with N linearly independent eigenvectors)
1043.	Which matrices can be decomposed by SVD? (Any matrix; V is either conjugate transpose or normal transpose depending on whether A is complex or real)
1044.	What is the trace of a matrix?
1045.	How to write Frobenius norm of a matrix A in terms of trace?
1046.	Why is trace of a multiplication of matrices invariant to cyclic permutations?
1047.	What is the trace of a scalar?
1048.	Write the frobenius norm of a matrix in terms of trace?
1049.	Numerical Optimization
1050.	What is underflow and overflow?
1051.	How to tackle the problem of underflow or overflow for softmax function or log softmax function?
1052.	What is poor conditioning?
1053.	What is the condition number?
1054.	What are grad, div and curl?
1055.	What are critical or stationary points in multi-dimensions?
1056.	Why should you do gradient descent when you want to minimize a function?
1057.	What is line search?
1058.	What is hill climbing?
1059.	What is a Jacobian matrix?
1060.	What is curvature?
1061.	What is a Hessian matrix?
1062.	Basics of Probability and Informaion Theory
1063.	Compare "Frequentist probability" vs. "Bayesian probability"?
1064.	What is a random variable?
1065.	What is a probability distribution?
1066.	What is a probability mass function?
1067.	What is a probability density function?
1068.	What is a joint probability distribution?
1069.	What are the conditions for a function to be a probability mass function?
1070.	What are the conditions for a function to be a probability density function?
1071.	What is a marginal probability? Given the joint probability function, how will you calculate it?
1072.	What is conditional probability? Given the joint probability function, how will you calculate it?
1073.	State the Chain rule of conditional probabilities.
1074.	What are the conditions for independence and conditional independence of two random variables?
1075.	What are expectation, variance and covariance?
1076.	Compare covariance and independence.
1077.	What is the covariance for a vector of random variables?
1078.	What is a Bernoulli distribution? Calculate the expectation and variance of a random variable that follows Bernoulli distribution?
1079.	What is a multinoulli distribution?
1080.	What is a normal distribution?
1081.	Why is the normal distribution a default choice for a prior over a set of real numbers?
1082.	What is the central limit theorem?
1083.	What are exponential and Laplace distribution?
1084.	What are Dirac distribution and Empirical distribution?
1085.	What is mixture of distributions?
1086.	Name two common examples of mixture of distributions? (Empirical and Gaussian Mixture)
1087.	Is Gaussian mixture model a universal approximator of densities?
1088.	Write the formulae for logistic and softplus function.
1089.	Write the formulae for Bayes rule.
1090.	What do you mean by measure zero and almost everywhere?
1091.	If two random variables are related in a deterministic way, how are the PDFs related?
1092.	Define self-information. What are its units?
1093.	What are Shannon entropy and differential entropy?
1094.	What is Kullback-Leibler (KL) divergence?
1095.	Can KL divergence be used as a distance measure?
1096.	Define cross-entropy.
1097.	What are structured probabilistic models or graphical models?
1098.	In the context of structured probabilistic models, what are directed and undirected models? How are they represented? What are cliques in undirected structured probabilistic models?
1099.	Confidence interval
1100.	What is population mean and sample mean?
1101.	What is population standard deviation and sample standard deviation?
1102.	Why population s.d. has N degrees of freedom while sample s.d. has N-1 degrees of freedom? In other words, why 1/N inside root for pop. s.d. and 1/(N-1) inside root for sample s.d.? (Here)
1103.	What is the formula for calculating the s.d. of the sample mean?
1104.	What is confidence interval?
1105.	What is standard error?
1106.	 
1107.	Project requirements change over time;

1108.	Cracking the Coding Interview - Gayle  Laakman
1109.	TopCoder - https://www.topcoder.com/ 
1110.	Coursera (Algorithmics with Princeton) https://www.coursera.org/learn/algorithmic-thinking-1
1111.	Codility - https://app.codility.com/demo/take-sample-test/
1112.	Khan Acadamy - https://www.khanacademy.org/computing/computer-science/algorithms
1113.	Data structures and algorithms - https://leetcode.com
1114.	System Design - https://github.com/donnemartin/system-design-primer
1115.	OOP and Design Patterns - https://www.youtube.com/playlist?list=PLF206E906175C7E07
1116.	Git - https://git-scm.com/book/en/v2 


1117.	What is sampled softmax?
1118.	Ans.	For Natural Language Processing or equivalent purposes, the target consists of large dimensionality. The vocabulary for language translation can be in 10,000s. Thus, the output from the neural network will be of large size for each datapoint. Computing softmax function over such a target is computationally intensive. To avoid this, sampled softmax technique introduces looking only at correct values in the output and a random sample of incorrect values, words in case of NLP. This is a subset of the target vector and an approximate value of loss function can be determined. Introduce by Sebastien Jean et al. in 2015. During inference, sampled softmax cannot be used, as it requires knowing the target. 
1119.	Using sampled softmax allows training models faster over a huge number of target classes, compared to usual softmax. This is used during training only, generating an underestimate of the full softmax loss. For inference softamx is applied using it on full set.

1120.	 

1121.	 
1122.	Credits: The fine Art of Small Talk; Author: Debra Fine
1123.	Describe a typical day on the job.
1124.	My job is to write code for Artificial Intelligence. I start by setting up. I go through my notes where I have jotted down what all I needed to complete for the day. Then I start coding. I keep calm, remembering to breathe every second. Then I use all my skills to complete the task. I communicate and socialize with colleagues to keep the work place a normal place rather making it a ghost house. At the end of the day I jot down important points for the next day. I generally dive into doing things and then rapidly correct the errors in my solution.
1125.	What got you started in this industry/area of practice?
1126.	I was introduced to this area during college and the course was an elective. I was not serious early after college with respect to this field. I worked in a company to gain experience in industrial practices. I did projects and was analyzing my profession. During this analysis I found that the profit margin in the field I was working was very low. It requires higher investment also which needs to be invested frequently. Then I analyzed what skills or areas, which I know, is investment friendly, be it financial, technical, ease of up-skilling or man hours effort. AI is the answer for all my questions. I want to reduce human effort and any type of loss or wastage.
1127.	What got you interested in research?
1128.	That is all I was taught during college and my first employment. So, I got used to it. When you advance your degrees you generally find yourself doing research and development type of work, where we either re-define an existing process or create something new through explore.
1129.	What do you enjoy most about your profession?
1130.	The opportunity to ease and re-define living across the human population is what keeps me excited.
1131.	What separates you and your firm from competition?
1132.	I currently do not work anywhere. I am trying to break through to have a relevant profession. I am good at completing things. My initial approach to dive into the solution with available knowledge sets my competitors far from completing projects. The main challenge is with a person who is an expert at his own game.
1133.	Describe some of the challenges of your profession.

1134.	 
1135.	https://www.geeksforgeeks.org/search-insert-and-delete-in-an-unsorted-array/
1136.	https://www.geeksforgeeks.org/search-insert-and-delete-in-a-sorted-array/
1137.	3.https://www.geeksforgeeks.org/write-a-c-program-that-given-a-set-a-of-n-numbers-and-another-number-x-determines-whether-or-not-there-exist-two-elements-in-s-whose-sum-is-exactly-x/
1138.	https://www.geeksforgeeks.org/two-pointers-technique/
1139.	https://www.geeksforgeeks.org/find-a-triplet-that-sum-to-a-given-value/
1140.	https://www.geeksforgeeks.org/find-the-number-occurring-odd-number-of-times/
1141.	https://www.geeksforgeeks.org/find-the-missing-number/
1142.	https://www.geeksforgeeks.org/count-frequencies-elements-array-o1-extra-space-time/
1143.	https://www.geeksforgeeks.org/median-two-sorted-arrays-different-sizes-ologminn-m/
1144.	https://www.geeksforgeeks.org/trapping-rain-water/
1145.	https://www.geeksforgeeks.org/nearly-sorted-algorithm/
1146.	https://www.geeksforgeeks.org/merge-one-array-of-size-n-into-another-one-of-size-mn/
1147.	https://www.geeksforgeeks.org/sort-array-wave-form-2/
1148.	https://www.geeksforgeeks.org/find-number-of-triangles-possible/
1149.	https://www.geeksforgeeks.org/minimum-de-arrangements-present-array-ap-arithmetic-progression/
1150.	https://www.geeksforgeeks.org/maximize-sum-arrii/
1151.	https://www.geeksforgeeks.org/merging-two-unsorted-arrays-sorted-order/
1152.	https://www.geeksforgeeks.org/check-reversing-sub-array-make-array-sorted/
1153.	https://www.geeksforgeeks.org/minimum-swaps-required-sort-binary-array/
1154.	https://www.geeksforgeeks.org/find-elements-larger-half-elements-array/
1155.	https://www.geeksforgeeks.org/find-one-extra-character-string/
1156.	https://www.geeksforgeeks.org/check-given-string-sum-string/
1157.	https://www.geeksforgeeks.org/count-strings-adjacent-characters-difference-one/
1158.	https://www.geeksforgeeks.org/string-slicing-python-rotate-string/
1159.	https://www.geeksforgeeks.org/run-length-encoding/
1160.	https://www.geeksforgeeks.org/reverse-words-in-a-given-string/
1161.	https://www.geeksforgeeks.org/remove-consecutive-vowels-string/
1162.	https://www.geeksforgeeks.org/length-of-the-longest-substring-without-repeating-characters/
1163.	https://www.geeksforgeeks.org/find-possible-words-phone-digits/
1164.	https://www.geeksforgeeks.org/number-distinct-permutation-string-can/
1165.	https://www.geeksforgeeks.org/count-palindrome-sub-strings-string/
1166.	https://www.geeksforgeeks.org/string-slicing-python-check-string-can-become-empty-recursive-deletion/
1167.	https://www.geeksforgeeks.org/converting-roman-numerals-decimal-lying-1-3999/
1168.	https://www.geeksforgeeks.org/recursively-remove-adjacent-duplicates-given-string/
1169.	https://www.geeksforgeeks.org/a-program-to-check-if-strings-are-rotations-of-each-other/
1170.	https://www.geeksforgeeks.org/binary-search-tree-set-1-search-and-insertion/
1171.	https://www.geeksforgeeks.org/binary-search-tree-set-2-delete/
1172.	https://www.geeksforgeeks.org/construct-bst-from-given-preorder-traversa/
1173.	https://www.geeksforgeeks.org/find-the-minimum-element-in-a-binary-search-tree/
1174.	https://www.geeksforgeeks.org/find-k-th-smallest-element-in-bst-order-statistics-in-bst/
1175.	https://www.geeksforgeeks.org/check-two-bsts-contain-set-elements/
1176.	https://www.geeksforgeeks.org/level-order-tree-traversal/
1177.	https://www.geeksforgeeks.org/diagonal-traversal-of-binary-tree/
1178.	https://www.geeksforgeeks.org/shortest-distance-between-two-nodes-in-bst/
1179.	https://www.geeksforgeeks.org/kth-ancestor-node-binary-tree-set-2/
1180.	https://www.geeksforgeeks.org/breadth-first-search-or-bfs-for-a-graph/
1181.	https://www.geeksforgeeks.org/depth-first-search-or-dfs-for-a-graph/
1182.	https://www.geeksforgeeks.org/applications-of-depth-first-search/, https://www.geeksforgeeks.org/applications-of-breadth-first-traversal/
1183.	https://www.geeksforgeeks.org/count-possible-paths-two-vertices/
1184.	https://www.geeksforgeeks.org/water-jug-problem-using-bfs/
1185.	https://www.geeksforgeeks.org/path-rectangle-containing-circles/
1186.	https://www.geeksforgeeks.org/bfs-disconnected-graph/
1187.	https://www.geeksforgeeks.org/detect-cycle-in-a-graph/
1188.	https://www.geeksforgeeks.org/detect-cycle-undirected-graph/
1189.	https://www.geeksforgeeks.org/find-length-largest-region-boolean-matrix/
1190.	https://www.geeksforgeeks.org/greedy-algorithm-egyptian-fraction/
1191.	https://www.geeksforgeeks.org/maximize-sum-arrii/
1192.	https://www.geeksforgeeks.org/sum-area-rectangles-possible-array/
1193.	https://www.geeksforgeeks.org/water-connection-problem/
1194.	https://www.geeksforgeeks.org/minimum-swaps-bracket-balancing/

1195.	 

1196.	Why do we NOT use Perceptron as often as a Logistic-Regression or Neural-Network? 
1197.	How does ReLU introduce non-linearity when it looks “linear”? It is easy to observe that Sigmoid introduces non-linearity as it is a nonlinear function. 
1198.	Why do we prefer dropout for regularization? Why not simply use L2 or L1 reg like in LogisticRegression? 
1199.	How does dropout work at test-time? 
1200.	Why is Max-pooling popular in CNNs? Why not any other function like mean, median, min etc? 
1201.	Why do individual learning rates per weight (like in AdaGrad) help as compared to one learning rate for all weights? 
1202.	Write the weight update functions for Adam.When does ADAM behave like Adadelta?
1203.	Number of parameters & hyper-params in a max-pooling layer? 9. How do we differentiate and back-prop through a max-pooling layer? 
1204.	You are training a model and its train loss is not changing from epoch to epoch. What could be the possible reasons? 
1205.	What’s the loss function of an autoencoder? 
1206.	Why is hierarchical softmax used in Word2Vec? 
1207.	How do we update weights in a Negative sampling based training of Word2Vec model? 
1208.	What are the trainable prams in a BatchNorm layer? 
1209.	Why is a ResNet able to learn models of significantly larger depth than earlier VGGNet? 
1210.	CS: How is back-prop related to dynamic programming in Algorithms? 
1211.	You have 5000 images with 5 class-labels and you want to build a CNN model? How would you go about building a classification model? 
1212.	Why does data augmentation help in object recognition tasks? 
1213.	Why does a GPU help in deep-learning much more than a multi-core CPU with say 8 or 16 cores? 
1214.	How is convolution different from simple weights as both involve element wise multiplication followed by addition? 
1215.	Derive the derivative of a sigmoid function? 
1216.	Why do we need Leaky ReLu? 
1217.	Why is tanh (sometimes) better than sigmoid for training a NN? 
1218.	How to fix exploding gradients in a MLP? 
1219.	You want to detect outliers using Neural Nets? How would you go about doing it?



